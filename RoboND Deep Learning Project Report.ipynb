{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project: Follow Me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Rubric Points](https://review.udacity.com/#!/rubrics/1067/view) \n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  \n",
    "\n",
    "I wrote it in the format of a Jupyter Notebook as usual, then exported as a markdown file. I will explain each rubric point moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The Network Architecture.  \n",
    "\n",
    "As the major task here for the network is to perform semantic segmentation, I decided to use a Fully Convolutional Networks. The basic structure is shown below.\n",
    "\n",
    "There are 3 components of the network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic structure](img/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Encoder <br></br><br></br>\n",
    "The encoder block has 3 convolutional layers. All of them are separable convolution layers, and their filter sizes increase with the depth of the network.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ec layer 1\n",
    "encoded_layer_1 = encoder_block(inputs, filters[\"ec1\"], stride_ec)\n",
    "    \n",
    "# ec layer 2\n",
    "encoded_layer_2 = encoder_block(encoded_layer_1, filters[\"ec2\"], stride_ec)\n",
    "    \n",
    "# ec layer 3\n",
    "encoded_layer_3 = encoder_block(encoded_layer_2, filters[\"ec3\"], stride_ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The 1x1 Convolution <br></br><br></br>\n",
    "This block performs 1x1 convolution and batch normalization. So it's kernel and stride are set to 1 directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_1by1_layer = conv2d_batchnorm(encoded_layer_3, filters[\"obo\"], kernel_size=1, strides=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decoder <br></br><br></br>\n",
    "The decoder block has 3 decode layers. For each of them, the first step is to perform a bilinear upsampling, with a factor for upsampling of 2, followed by a layer concatenation step. After that, 1 more separable convolution layer is added to provide some more spatial information. The decode layers are concatenated with the encode layers in a reversed sequence, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dc layer 1\n",
    "# concat with ec layer 3\n",
    "decoded_layer_1 = decoder_block(conv_1by1_layer, encoded_layer_3, filters[\"dc1\"])\n",
    "\n",
    "# dc layer 2\n",
    "# concat with ec layer 2\n",
    "decoded_layer_2 = decoder_block(decoded_layer_1, encoded_layer_2, filters[\"dc2\"])\n",
    "\n",
    "# dc layer 3\n",
    "# concat with ec layer 1\n",
    "decoded_layer_3 = decoder_block(decoded_layer_2, encoded_layer_1, filters[\"dc3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above structure the filter sizes and stride size are defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters = {\n",
    "    \"ec1\": 16,\n",
    "    \"ec2\": 32,\n",
    "    \"ec3\": 64,\n",
    "    \"obo\": 128,\n",
    "    \"dc1\": 64,\n",
    "    \"dc2\": 32,\n",
    "    \"dc3\": 16\n",
    "}\n",
    "\n",
    "stride_ec = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter sizes for the encoder block is a recommended one in previous deep learning labs, and the decoder block just uses filter sizes that match them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the Network\n",
    "\n",
    "I was using the data provided in the project to train my network.\n",
    "\n",
    "**Hyperparameters** <br></br>\n",
    "\n",
    "Define the hyperparameters.\n",
    "- **batch_size**: number of training samples/images that get propagated through the network in a single pass.\n",
    "- **num_epochs**: number of times the entire training dataset gets propagated through the network.\n",
    "- **steps_per_epoch**: number of batches of training images that go through the network in 1 epoch. \n",
    "- **validation_steps**: number of batches of validation images that go through the network in 1 epoch. This is for the validation dataset.\n",
    "- **workers**: maximum number of processes to spin up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will present my tuning process as examples and explain it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 1: \n",
    "\n",
    "**On local machine**\n",
    "\n",
    "The parameters are set as the original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "batch_size = 64\n",
    "num_epochs = 2\n",
    "steps_per_epoch = 200\n",
    "validation_steps = 50\n",
    "workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This the first run after the model is built. The purpose is to test if the network is work properly without a bug, so I didn't change any of the provided paramters. \n",
    "\n",
    "**Result**\n",
    "\n",
    "The network runs without a bug, but failed the evalution miserably. The IoU for hero and other people are both 0.0, and it took a very long time to train. According to the printout, it took about 1800 seconds to finish just 1 epoch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "In my opinion, there can be two main reasons why the IoU for hero and other people are so low:\n",
    "\n",
    "1. The architecture is wrong, or not deep enough\n",
    "2. The choices of the parameters are poor\n",
    "\n",
    "Since this is just the first run, I think it would be more logical to try to fine tune the parameters instead of changing the architecture directly. Most importantly, there are only 2 epochs, and I can almost be certain that it wouldn't be enough. So I performed a second run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 2: \n",
    "\n",
    "**On local machine**\n",
    "\n",
    "The parameters are set as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 20\n",
    "num_epochs = 10\n",
    "steps_per_epoch = 200\n",
    "validation_steps = 50\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Tweaking**\n",
    "\n",
    "- For the learning rate, as we are taught in the lessons, \"if you don't know where to start, always lower the learning rate\". \n",
    "- I decreased the batch size based on the fact that it took a very long time for my computer to complete 1 epoch, which means it's probably reaching its limit computing with the previous batch size.\n",
    "- As indicated in the lesson, more epoch usually means better performance. So I increased it to 10. I didn't go for a larger number, however, because I would like to make sure that the tweaking of the parameters indeed provide improvement to my network.\n",
    "- I have also increased the number of workers hoping it will increase the computing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "This time the network performs better. It still took a very long time to train, (about 1700 seconds to finish 1 epoch) but the IoU for hero and other people are no longer 0.0. The performance is still very poor, but this means the tweaking is working, and I should change the parameters more drastically along current direction. But my local machine apparently can no longer handle the task, so I uploaded the project to my AWS instance and performed all following runs from there. \n",
    "\n",
    "A final training curve for this run is attached below. The final results are: \n",
    "\n",
    "loss: 0.2727 - val_loss: 0.3363\n",
    "\n",
    "![training curve](img/localRun2.png)\n",
    "            \n",
    "\n",
    "A full record of the run and training curves can be found [here.](pdf/LocalRun2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 3: \n",
    "\n",
    "**On AWS instance**\n",
    "\n",
    "The parameters are set as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "steps_per_epoch = 65\n",
    "validation_steps = 50\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Tweaking**\n",
    "\n",
    "On AWS instance I am no longer limited by the hardware, at least for the computation needed for this project. So I went wild on the parameter settings:\n",
    "- Lower the learning rate further more. \n",
    "- Increase the batch size to speed up processing. \n",
    "- Previous run on my local machine already proves that more epoch **is** helping a lot. So I increased it to 50.\n",
    "- Noticing the fact that the data set contains only about 4000 images, I set the steps per epoch to be the total number of images divided by the batch size: 65.\n",
    "- I have also doubled the number of workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "The network continues to improve. Thanks to the powerful AWS instance, it now only take about 120 seconds to finish 1 epoch, which is more than 10 times faster than local computing.  \n",
    "\n",
    "The scores in different scenarios are also improved. More specifically:\n",
    "\n",
    "1. While the quadrotor is following behind the target:\n",
    "    - IoU for background is 0.99\n",
    "    - IoU for other people is 0.30\n",
    "    - IoU for hero is 0.85\n",
    "\n",
    "   Out of 542 validation samples, the network got 539 true positives.\n",
    "   \n",
    "2. While the quadrotor is on patrol and the target is not visible:\n",
    "    - IoU for background is 0.98\n",
    "    - IoU for other people is 0.59\n",
    "    - IoU for hero is 0.0\n",
    "\n",
    "   Out of 270 validation samples, the network got 119 false positives.\n",
    "   \n",
    "3. While the hero is far away and the network needs to detect the target:\n",
    "    - IoU for background is 0.99\n",
    "    - IoU for other people is 0.40\n",
    "    - IoU for the hero is 0.23\n",
    "    \n",
    "   Out of 322 validation samples, the network got 154 true positives, and also 147 false negatives.\n",
    "   \n",
    "4. Weight for the score:\n",
    "    0.72\n",
    "    \n",
    "5. Final IoU:\n",
    "    0.54\n",
    "    \n",
    "6. Final score:\n",
    "    0.39\n",
    "\n",
    "A full record of the run and training curves can be found [here.](pdf/Run3.pdf)\n",
    "\n",
    "The weights are saved under name \"run3_aws\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysis**\n",
    "\n",
    "From the scores above, I can see that the network performs best when following directly behind the target, but falls short on the other two scenarios. All the ups and downs on the performances are within reasonable range, and the final score is very close to the targetting score of 0.4. So I decided to train it even longer without changing any other parameters to see if it can promote performance even further.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 4: \n",
    "\n",
    "**On AWS instance**\n",
    "\n",
    "The parameters are set as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "steps_per_epoch = 65\n",
    "validation_steps = 50\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Tweaking**\n",
    "\n",
    "As I mentioned above, the only change I made was to double the number of epochs. In a separated test run I have also doubled the number of workers to 16, trying to speed up the computation even more, but the time it took to complete one epoch was still around 120 seconds. So I just keep the number of workers to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "The network continues to improve, but the improvement is not as huge as before. Also, judging by the training curves, it seems like that the network is overfitting the data. \n",
    "\n",
    "The final score is indeed promoted to about 0.45, but I have a bad feeling about the training curves. So I decreased the number of epochs to 75 for the next run. \n",
    "\n",
    "A full record of the run and training curves can be found [here.](pdf/Run4.pdf)\n",
    "\n",
    "The weights are saved under name \"run4_aws\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run 5: \n",
    "\n",
    "**On AWS instance**\n",
    "\n",
    "The parameters are set as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 75\n",
    "steps_per_epoch = 65\n",
    "validation_steps = 50\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Tweaking**\n",
    "\n",
    "As I mentioned above, the only change I made was to decrease the number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result**\n",
    "\n",
    "The performance and scores went down a little bit, but the training curves were better this time, no more overfitting. \n",
    "\n",
    "The final score is slightly lower at about 0.43, but still considered acceptable, without overfitting. \n",
    "\n",
    "A full record of the run and training curves can be found [here.](pdf/Run5.pdf)\n",
    "\n",
    "The weights are saved under name \"run5_aws\".\n",
    "\n",
    "And that concludes the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Techniques and Concepts\n",
    "\n",
    "A 1x1 convolution layer is used in the network, mainly because we need to perform semantic segmentation. For that purpose, the spacial information needs to be retained. This is lost when we connect the output of a convolutional layer to a fully connected layer and flatten it into a 2D tensor. Hence, a fully connected layer is replaced by the 1x1 convolution layer, and then the output is feed into the decoder block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Image Preprocessing\n",
    "\n",
    "As I mentioned above, all the data used to train my network come provided in the project. But as my result shows, the network is not doing very well when the target is not visible or very far away, so a way to improve this it to collect more data in this scenario to train the network. \n",
    "\n",
    "In the data collection process, the images are all stored raw, and they first need to be preprocessed. Besides converting the images from .png to .jpeg for uploading to AWS, a more important task in the preprocessing step is to transform the depth masks from the sim into binary masks. More specifically, it is to generate the images in the training and masks folders as we see in the provided data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Future Improvements\n",
    "\n",
    "- I have used 3 layers in both encoder and decoder blocks. Theoretically, a deeper network with 4 layers or more should further improve the result.\n",
    "- As shown in my runs, when the number of epochs are set too large overfit will begin to emerge. So instead of increasing it other hyper paramters can be further tweaked to improve the result.\n",
    "- A well collected and larger data set is almost always better for the training. I have produced a large collection of data hoping to train my network for patrolling without target and detecting target from far away, but never put into use because all my masks were rendered fully with background without any target or other people. I think it was due to the placement of my patrol path being too far away from hero path and people spawn spots, and the quadrotor can hardly see them. In the future a better collected data set should be able to promote performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Limitations and Changes\n",
    "\n",
    "- Currently, all the images collected in the train data are modeled after **people**, so if the target is replaced by another kind of object, the network might very well be failing. \n",
    "\n",
    "- Noticing the fact that the major part in the recognition is semantic segmentation, which means we only care about the shape and place, not other properties of the target, i.e. color, material, components, surface smoothness, the network architecture might actually work well with other objects, the only change is the train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
